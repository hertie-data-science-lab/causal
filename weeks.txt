---
title: "Foundations"
date: "September 8th, 2020"
description: "Mechanisms and Counterfactuals"
---

This week we review the counterfactual theory of causation and the
kinds of causal inferences it licenses. We'll compare and contrast two
complementary views of a causal mechanism: as a directed acyclic graph
(DAG) that connects variables and implies statistical independence
relations between them, and as independence relations that connect the
potential outcome (PO) for each of these variables that individual
cases can realize.

It is often helpful to think of POs as the individual level
implications of a DAG, where some implications are about what
did not happen but could have done had things gone differently.

In the background of each way of thinking about causation is the
idea that there is a set of specific functions that
realize the arrows in the DAG and are the reason that the POs
have whatever structure they do. Since we seldom have strong ideas about
what these functions look like, it is often useful to ask whether
summaries of a functional relationship at particular values
of the variables can be identified from data without knowing the
functional forms in advance. This is the question of non-parametric
causal identification, for which DAGs were designed. We can also
ask about the minimal assumptions about functional form required,
for example that some relationship is monotonically increasing.
For this, switching to POs becomes necessary.


## Readings (for weeks 1 *and* 2)

- WhatIf ch. 1, 2, 3, 6, and 10

- G. W. Imbens (2020) [Potential Outcome and Directed Acyclic
  GraphApproaches to Causality: Relevance for Empirical Practice in
  Economics](https://arxiv.org/pdf/1907.07271.pdf) Arxiv 1907.07271v2
  (A graph skeptical overview of this week's topics from an economist)

- Steiner et al. 2017 [Graphical models for quasi-experimental designs](https://dl.conjugateprior.org/ciml/reading/Steineretal2017.pdf) Sociological Methods and Research.

## Lecture

- [Moodle page](https://moodle.hertie-school.org/course/index.php?categoryid=266)

- [slides](lectures/week01.pdf) for reference

At 1.45 hours this is frankly much too long. I'll try to talk faster
or say less in the future.

 
---
title: "Experiments, Quasi-experiments, and Definitely not Experiments"
date: "September 8th, 2020"
description: "What's so great about randomization and control anyway?"
---

An important question to ask for any study design is: what causal
effects are, or could be identified here? Often the answer is 'none',
and that is an important thing to know. Not only is purely descriptive
research important in itself, but all causal inferences from data are
still descriptive studies when the causal claims are removed. 

More typically we must ask: what assumptions - in the broadest sense -
would it be necessary to make in order to identify an effect of
interest in this study?

This week we revisit the 'experimental ideal' for causal inference
with a critical eye. In most policy contexts we cannot, or should not,
randomize or control 'treatment' assignment, so one question is how
close we can get to the ideal.

Conversely, when we *do* manage to get an experiment running, particularly 
a field experiment, then lots of things can happen that make our happy 
randomization and control go wrong, e.g. non-compliance, attrition ('drop out') and 
missing data of a less drastic variety. At this point we have an partly 
'observational' study again, so we may as well get used to it.

Either way it'll be useful to work in a framework that doesn't strongly 
distinguish experimental and non-experimental work. The general issue 
will be: how *close* to the experimental ideal can we get?

Finally we will consider methods of adaptive experimentation where the 
sample size and design is dynamically allocated. This methodology was 
developed in computer science for 'active', and more generally 'reinforcement 
learning' applications, but is now used for large scale experimentation, e.g. 
in the tech sector. We will look at an example using Thompson sampling.

## Readings (for weeks 1 *and* 2)

- A. S. Gerber and D. P. Green (2012) 'Field experiments: Design,
  analysis, and interpretation' Norton. (highly recommended for those
  new to field experiments)

- EGAP's [methods guides](https://hertie-data-science-lab.github.io/causal-inference/week02.html) on many of the topics from lecture. Well worth bookmarking.
  Mostly development examples.

- G. W. Imbens (2020) [Potential Outcome and Directed Acyclic
  Graph Approaches to Causality: Relevance for Empirical Practice in
  Economics](https://arxiv.org/pdf/1907.07271.pdf) Arxiv 1907.07271v2
  (A graph skeptical overview of this week's topics from an economist)

- WhatIf ch. 1, 2, 3, 6, and 10

- Steiner et al. 2017 [Graphical models for quasi-experimental designs](https://dl.conjugateprior.org/ciml/reading/Steineretal2017.pdf) Sociological Methods and Research.

- Russo et al. 2017 [A tutorial on Thompson sampling](https://arxiv.org/abs/1707.02038) 
  arXiv 1707.0203.
  
- A quick [worked example](https://dl.conjugateprior.org/ciml/thompson.html) 
  of Thompson sampling for adaptive ATE estimation

## Lecture

- [Lecture download](https://www.dropbox.com/s/c7ba1j9en6j0j8d/ciml-week02.mov?dl=0)

- [Moodle page](https://moodle.hertie-school.org/course/index.php?categoryid=266) (not there yet)

- [slides](lectures/week02.pdf)



---
title: "Stratification, Regression, and all that"
date: "September 8th, 2020"
description: "When is a regression a good plan for causal inference? Not always, it turns out."
---

This week we cast our citical eye on our old friend, the regression
model. It is helpful to think about regression models as a more
sophisticated form of stratification, but they do more than that.  And
also less. Knowing what they actually do from a causal inference
perspective makes us a little more cautious, but also more realistic
about what regression can offer us.

In particular, we may want to use them indirectly, for example to
generate weights such as propensity scores, or to combine different
regression models in the same analysis, for example to study questions
of mediation.

For any regression model we'll also ask want to ask two big questions:
First, since not everything that should be conditioned on is in this
regression model, how *sensitive* are causal inferences to what is
not observed. Second, should everything that is in the model be
there. For example, conditioning on collider variables will wreck
the most otherwise careful analysis and, as often, there will be no
statistical warning that anything has gone wrong. We'll consider a general 
typology of controls, good and bad, and the unwiseness of interpreting 
the 'effects' of control variables.

We'll also ask about the *representativeness* of regression estimators 
of causal effects when they are heterogeneous and note that here, 
perhaps surprisingly, some cases are more influential than others.

## Readings

- WhatIf: ch. 11, 12, 13, 15

- Aronow and Samii (2016) Does regression produce representative estimates of 
  causal effects? American Journal of Political Science.
  
- Cinelli and Hazlitt (2020) [Making sense of sensitivity: extending omitted variable bias](https://carloscinelli.com/files/Cinelli%20and%20Hazlett%20(2020)%20-%20Making%20Sense%20of%20Sensitivity.pdf)

- Cinelli et al. (2020) [A crash course in good and bad controls](https://carloscinelli.com/files/Cinelli%20et%20al%20(2020)%20-%20A%20Crash%20Course%20in%20Good%20and%20Bad%20Controls.pdf)

- Keele et al (2020) [The causal interpretation of estimated associations in regression models](https://pdfs.semanticscholar.org/dc9f/889b4931f54f7538d641d10176442b76277c.pdf) Political Science and Research Methods

- King and Nielsen (2019), [Why Propensity Scores Should Not Be Used for Matching](https://doi.org/10.1017/pan.2019.11) Political Analysis

## Lecture

[Link](https://moodle.hertie-school.org/course/index.php?categoryid=266)

## Assignment

[Link](https://rstudio.cloud/)
---
title: "Machine Learning and Big Data Changes Everything"
date: "September 8th, 2020"
description: "Or maybe it doesn't. Let's see"
---

This week we see how our causal inferences could be helped or hindered
by taking advantage of more powerful tools for regression from
machine learning, and making use of increasingly massive data sets.

More powerful models usually means more 'flexible' models so it will
be important to see what the costs and benefits of non-linear
regression are for causal inference. More flexible models allows us
to go wrong more drastically, so questions of model fitting and
overfitting will be important here.

Bigger data will certainly make our inferences more precise, but it is
an open question whether it can make us righter. Identifying a causal
effect is the situation where, if we had all the data we could possibly
want then some quantity estimated from a model would be the causal
effect we are interested in. Conversely, if the effect is not identified
then no amount more data will make it so. So we might be skeptical
that more precision in our estimation due to more data will help. However,
in policy domains we are often interested in a causal effect in some
groups more than in others, so more data may help us identify these
group effects better. Unless we overfit the model, in which case our
precision is an illusion.


## Readings

- WhatIf: ch. 11 and 18

- Meng 2018 'Statistical paradises and paradoxes in big data (I): Law of large 
  populations, big data paradox, and the 2016 US presidential election' Annals 
  of Applied Statistics.
  
- Chetverkov 2016 'Double Machine Learning for Causal and Treatment Effects', 
  see also (Felton on 
  https://scholar.princeton.edu/sites/default/files/bstewart/files/felton.chern_.slides.20190318.pdf) for an accessible introduction.
  
- Hernan et al. (2019) [1 January 14, 2019 Data science is science's second chance to get causal inference right. A classification of data science tasks](https://arxiv.org/ftp/arxiv/papers/1804/1804.10846.pdf)

## Lecture

[Link](https://moodle.hertie-school.org/course/index.php?categoryid=266)

## Assignment

[Link](https://rstudio.cloud/)
---
title: "Old School Tools from the Economists (1)"
date: "September 8th, 2020"
description: "Fixed effects, difference in differences, and trouble"
---

This week we will cast our analytical eye on the traditional study
designs you will meet in applied economics and policy analysis. We
will focus on identifying the assumptions they require to do causal
inference for us, and on extending them to more complicated policy
scenarios than they were initially designed for.

Everybody knows, and some even love 'fixed effects' and lagged
variables as a tool for dealing with confounding that would otherwise
compromise our causal inferences. But is is surprisingly difficult to
see what we have to assume for them to work. When we do, they can seem
rather less innocuous. We'll take a look.

We will also investigate what happens to 'difference in difference'
analyses when units, e.g. countries or states, receive their
'treatment' at different times. This is a very common and important
situation to study, but the standard tools need adjusting for it.

## Readings

- Goodman-Bacon 2018 'Difference-in-Differences with Variation in
  Treatment Timing' NBER Report w25018.

- Kim and Steiner 2019 'Causal Graphical Views of Fixed Effects and
  Random Effects Models' MS

- Bell and Jones 2014 'Explaining fixed effects: Random effects
  modeling of time-series cross-sectional and panel data' Political
  Science Research and Methods.

- Imai and Kim 2019 'When Should We Use Unit Fixed Effects Regression
  Models for Causal Inference with Longitudinal Data?' American
  Journal of Political Science.

- Bellemare et al (2017) Lagged Explanatory Variables and the Estimation of 
  Causal Effects. Journal of Politics
  
## Lecture

[Link](https://moodle.hertie-school.org/course/index.php?categoryid=266)

## Assignment

[Link](https://rstudio.cloud/)
---
title: "Old School Tools from the Economists (2)"
date: "September 8th, 2020"
description: "Instrumental variables and regression discontinuities"
---

This week we continue to making life more interesting for
econometricians' favourite methods by looking into instrumental
variables and regression discontinuity designs. Both require stronger
assumptions for causal inference than other tools, primarily because
we must make functional form assumptions as well as regular causal
assumptions.

Policy domains often seem to offer us, for example in the form of
accidents of geography or laws being passed, some convenient
discontinuities around which we can organize a causal inference. But
we should be cautious in a situation where treatment and control have
no overlapping cases, an effect is defined only at the discontinuity,
and we almost always end up making functional form assumptions that
affect our inferences. Moreover, if the discontinuity is not relevantly
exogenous, or if actors see the discontinuity coming and can sort
themselves onto one or other side of it, our inferences may be
compromised.

Similarly, in realistically heterogenous populations, instrumental
variables offer us a causal effect that is specific to an unknown
proportion of the population sensitive to the instrument, but we must
assume rather specific structure to their reactions to it, as well as
the absence of confounding with the outcome. In policy domains these
difficulties are particularly acute where exclusion restriction arguments 
are often hard to make. Fortunately we can extend the notion 
of instrument to conditional instruments, which may not - weirdly - 
even have to cause treatment.

## Readings

- Imbens 2014 'Instrumental variables: An econometrician's
  perspective' Statistical Science.
  


## Lecture

[Link](https://moodle.hertie-school.org/course/index.php?categoryid=266)

## Assignment

[Link](https://rstudio.cloud/)
---
title: "Collider Bias in Theory and Practice"
date: "September 8th, 2020"
description: "Not everything is a confounder, so do not condition on all the things"
---

We spend this week thinking about 'collider bias' and its
consequences.

Collider bias provides a particularly illuminating framework for
understanding why all the things you were told not to do in your last
statistics class were bad. Examples, include selection on the dependent
variable, conditioning on post-treatment outcomes, and generally pointing
statistical models at data whose provenance you are unsure of.

Collider bias is a particular issue for policy students as much of
their data comes from organizations, who collected it for reasons
unrelated but often unexpectedly relevant to the purpose you want it
for. In particular, administrative data is usually created in response
to some event, for example a birth, doctor's appointment, accident, or
arrest. Colliders will be particulary troublesome when we try to
answer some questions with this kind of non-randomly sampled data.

## Readings

- WhatIf ch. 8

- Hernan et al. 2004 'A Structural Approach to Selection Bias' Epidemiology.

- Elwert and Winship (2014) 'Endogenous selection bias: The problem of
  conditioning on a collider variable' Annual Review of Sociology.


## Lecture

[Link](https://moodle.hertie-school.org/course/index.php?categoryid=266)

## Assignment

[Link](https://rstudio.cloud/)
---
title: "Mediation, of the Statistical Variety"
date: "September 8th, 2020"
description: "It's harder than you think to learn about causal mechanisms"
---

This week we ask the question: if there are several ways to generate a
policy outcome, how important is each route? This is the mediation
analysis, beloved of psychologists and communication scholars and,
we shall see, rather more difficult than they hope.

One of the most important questions we can ask when trying to unpack
the idea of one 'route' to an outcome being more important than
another, is: what precisely is the causal effect of interest? Perhaps
unexpectedly, the answers to this question require some interesting -
or dismaying depending on your preferences - metaphysical
considerations. For example, is the effect of a job finding
program on labour force outcome more mediated by the increasing confidence
or improving skills?

Assuming for a moment that these are independent, is the causal effect
of the confidence 'route' the difference between your labour market
outcome when you get treated, your confidence is raised, and your
skills improved, and the counterfactual you whose skills are improved
because you were treated, but whose confidence is not increased,
exactly as if you were not treated? And if so, who is this person who
is and is not affected by the treatment?

Here we are only trying to define the causal effects; things get
harder as we try to estimate it.



## Readings

- Keele (2015) 'Causal mediation analysis: Warning! Assumptions ahead'
  American Journal of Evaluation. *or*

- Green et al. (2010) 'Enough already about "black box" experiments:
  Studying mediation is more difficult than most scholars suppose' The
  Annals of the American Academy of Political and Social Science. *or*

- Bullock et al. (2010) 'Yes, but what’s the mechanism? (Don’t expect an
  easy answer)' Journal of Personality and Social Psychology. *or*

- Imai et al. (2010) 'A general approach to causal mediation analysis'
  Psychological Methods.
  
- Acharya et al. (2015) 'Explaining causal findings without bias: Detecting and 
  assessing direct effects'
  

## Lecture

[Link](https://moodle.hertie-school.org/course/index.php?categoryid=266)

## Assignment

[Link](https://rstudio.cloud/)
---
title: "Fairness and Bias in Algorithms and Humans"
date: "September 8th, 2020"
description: "If fairness is a counterfactual concept then lots of things can be biased"
---

This week we look at fairness and its evil twin bias. Our starting
point will be, roughly, that fairness as an idea is essentially
counterfactual. Specifically, I or my algorithm treat you fairly in an
allocation decision with respect to one of your attributes, for
example your gender, if I would have allocated you the same thing had
your gender been different.

While not the only way to think about fairness, this has some
intuitive appeal. However, many other things about you would have been
caused to be different had your gender been different, even assuming
the counterfactual 'you with another gender' is conceivable. Things
get interestingly difficult rather quickly and much legal and moral
argument depends implicitly on answers to the ensuing difficulties,
for example the concrete question about whether fairness requires that
no allocation model depends on your gender. We will examine
circumstances where this simple heuristic may fail as well as
situations where competing definitions of fairness are in principle
incompatible. We will try to use our causal inference tools to make
some sense of the issue.

## Readings

- Kusner et al. (2017) 'Counterfactual Fairness' Advances in Neural
  Information Processing Systems.

- Russell et al. (2017) 'When worlds collide: Integrating different
  counterfactual assumptions in fairness' Advances in Neural
  Information Processing Systems.

- Pleiss et al. (2017) 'On Fairness and Calibration'. Arxiv

## Lecture

[Link](https://moodle.hertie-school.org/course/index.php?categoryid=266)

## Assignment

[Link](https://rstudio.cloud/)
---
title: "Fairness and Bias: Case Study"
date: "September 8th, 2020"
description: "A deep dive into an important policy question"
---

This week we take a in-depth look at the question of whether and if
so, to what extent, US police are biased against monorities in their
use of force. We will base the session around a recent paper on the
topic in the light of its precursors and subsequent controversy.

This issue will encompass many of the causal inference issues dealt
with in the course, and provides an important policy-relevant example
of multiple causal inference tools at work.

## Readings

- Knox et al. 2020 'Administrative Records Mask Racially Biased
  Policing' American Journal of Political Science.
  
- Gaebler et al. [Deconstructing Claims of Post-Treatment Biasin Observational Studies of Discrimination](https://5harad.com/papers/post-treatment-bias.pdf)

- Knox et al [Can Racial Bias in Policing Be Credibly EstimatedUsing Data Contaminated by Post-Treatment Selection?](https://dcknox.github.io/files/KnoxLoweMummolo_PostTreatmentSelectionPolicing.pdf) MS

## Lecture

[Link](https://moodle.hertie-school.org/course/index.php?categoryid=266)

## Assignment

[Link](https://rstudio.cloud/)
---
title: "Special topics: Sensitivity and Bounds"
date: "September 8th, 2020"
description: "When you just don't know, sometimes you should offer all possible answers"
---

In this session we consider how to to gauge the robustness of causal
inferences to things we know we don't know, but suspect might
matter. This process is called sensitivity analysis and is not
practiced as much as you might hope.

This is distinct from what are referred to as 'robustness checks'
because rather than investigating alternative functional form of or
additional measured variables, it asks: how big do the effects of all
unmeasured confounders have to be before the effect I currently have
disappears.

We will also look at tools to bound causal effects. Suprisingly often
observed data will imply logical constraints on how big or small a
causal effect could possibly be. Applying these can often be
informative, even without a traditional causal identification. 

## Readings

TBA

## Lecture

[Link](https://moodle.hertie-school.org/course/index.php?categoryid=266)

## Assignment

[Link](https://rstudio.cloud/)
---
title: "Special Topics: Alternative Approaches to Causal Inference"
date: "September 8th, 2020"
description: "There's more than one way to do it. Well, maybe."
---

This week we will consider some supposed alternatives to the
counterfactual theory of causation we have been assuming, and see what
the we would do differently under them.

We will also consider 'process tracing' as a qualitative tool for
investigating causal mechanisms, as a precursor to a general
discussion of how we might use additional information in our causal inferences,
whether qualitative or quantitative.

## Readings

- Beach [Process-Tracing Methods in Social Science](https://oxfordre.com/politics/view/10.1093/acrefore/9780190228637.001.0001/acrefore-9780190228637-e-176) (old school PT)

- Bennett (2008) 'Process tracing: A Bayesian perspective.' The Oxford handbook of political methodology

## Lecture

[Link](https://moodle.hertie-school.org/course/index.php?categoryid=266)

## Assignment

[Link](https://rstudio.cloud/)
