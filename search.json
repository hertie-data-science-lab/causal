{
  "articles": [
    {
      "path": "index.html",
      "title": "Causal Inference",
      "description": "Contemporary approaches to causal inference and its application to policy questions\n",
      "author": [],
      "contents": "\nInstructor:\nDr. William E. M. Lowe\nOffice:\nRoom 3.14\nOffice Hours\nBy arrangement. Email the instructor directly.\nClass Times\nTuesdays 16:00-18:00\nClassroom\n2.30\nMoodle\nCourse page\nSessions\n\nDate\nTitle\n1\n08.09.2020\nFoundations\n2\n15.09.2020\nExperiments, Quasi-experiments, and Definitely not Experiments\n\n\nAssignment 1 out\n3\n22.09.2020\nStratification, Regression, and all that\n4\n29.09.2020\nMachine Learning and Big Data Changes Everything\n\n\nAssignment 2 out\n5\n06.10.2020\nEven More Machine Learning\n\n\nAssignment 3 out\n6\n13.10.2020\nHalf Time Review, plus some Diff-in-Diff\n\n\nAssignment 4 out\n\n\n \n7\n27.10.2020\nCollider Bias in Theory and Practice\n8\n03.11.2020\nMediation, of the Statistical Variety\n9\n10.11.2020\nFairness and Bias in Algorithms and Humans\n10\n17.11.2020\nFairness and Bias: Case Study\n11\n24.11.2020\nSpecial topics: Sensitivity and Bounds\n\n\nAssignment 5 out\n12\n01.12.2020\nSpecial Topics: Alternative approaches to Causal Inference\n\n14.12.2020\nFinal Exam Week\nLinks to textbooks (not required)\nHernán Miguel A. and Robins James M. (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.\nPearl, Judea, Glymour, Madelyn, and Jewell, Nicholas 2016. Causal Inference in Statistics: A Primer. Wiley.\nAngrist, Joshua D. and Pischke, Jörn-Steffen 2009 Mostly Harmless Econometrics. Princeton University Press\n\n\n\n",
      "last_modified": "2021-03-23T09:13:15+01:00"
    },
    {
      "path": "syllabus.html",
      "title": "Syllabus",
      "description": "Your single source of course information",
      "author": [],
      "contents": "\nLogistics\n \n\nInstructor:\nDr. William E. M. Lowe\nAssistant Email:\nadjunctsupport@hertie-school.org\nOffice:\nRoom 3.14\nOffice Hours\nBy arrangement. Email the instructor directly.\nClass Times\nTuesdays 16:00-18:00\nClassroom\n2.30\nMoodle\nCourse page\nFormat\nThe course will consist of short pre-recorded lectures from the instructor and live discussion of these conducted either in person, remotely, or a mix of both. There are regular exercises to be submitted the week after they are set. Students will finish the course with an in depth analysis of existing substantively-focused instructor-approved paper of their choice.\nGeneral Readings\nThere are no required textbooks, but chapters from\nHernán Miguel A. and Robins James M. (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.\nwill be noted in the weekly readings for reference.\nStudents who have taken Statistics 2 may have a copy of\nPearl, Judea, Glymour, Madelyn, and Jewell, Nicholas 2016. Causal Inference in Statistics: A Primer. Wiley.\nAngrist, Joshua D. and Pischke, Jörn-Steffen 2009 Mostly Harmless Econometrics. Princeton University Press\nwhich will cover a subset of the materials we do, but with a rather different focus.\nPrerequisites\nStatistically, students should be familiar with fitting and interpreting linear models. This course will treat similar topics to Statistics 2, so having taken that course will be an advantage, but will focus more on conceptual issues and on literature outside political science and economics, e.g. machine learning.\nPractically, students should be competent, though need not be expert, with the sampling functions and simple data.frame manipulations in R.\nGrading and Assignments\nThe course grade is composed of five take home data analysis exercises, worth 10% each, a final report worth 40%, and participation grade worth 10%\nExercises\nData analysis exercises are guided practical exercises based around a data set or text collection, interspersed with conceptual questions about the tools being used, interpretation of results, etc. The exercises are designed to improve practical skills and test knowledge from lectures and reading. Grading is based on success in the practical components and the quality of written answers. Answers to conceptual questions are intended to require at most several paragraphs of text. Note that different exercises have a different mixes of practical and explanatory / analytical requirements.\nFinal Report\nFor the final report, students choose a substantively oriented paper on a policy topic of their choice, subject to instructor approval, and provide both a critique of its methods from a causal perspective, and motivate a set of proposals to remedy any identified defects (if that is possible), e.g. by providing alternative or additional research design or other analytical strategies to address the original research question. Note that there is no requirement that suggestions must use exactly the same data but suggestions for alternative data sources must be reasonable.\nParticipation\nThe participation grade is based on the assumption that students take part, not as passive consumers of knowledge, but as active participants in the exchange, production, and critique of ideas—their own ideas and the ideas of others. Therefore, students should come to class not only having read and viewed the materials assigned for that day but also prepared to discuss the readings of the day and to contribute thoughtfully to the conversation. Some course materials can be difficult, not least because we will often be reading recently written articles close to the state of the art, without the benefit of year of expository practice. Consequently, it is important to identify and be honest about what you have understood and what remains unclear as you participate in the class. In particular, the instructor does not expect, although would be delighted by, a complete technical understanding of every paper before class.\nParticipation is graded subtractively; students receive the full grade except to the extent they fail to take adequate part in the class. Participation is marked by its active nature, its consistency, and its quality, but note that it is both unnecessary and also unwise, to monopolize conversation in order to maximize participation grade. Participation that makes it harder for other class members to engage in discussion will lead to a lower grade, regardless of the quality of interventions.\nLate submission of assignments\nFor each day the assignment is turned in late, the grade will be reduced by 10% (e.g. submission two days after the deadline would result in 20% grade deduction).\nAttendance\nStudents are expected to be present and prepared for every class session. Active participation during lectures and seminar discussions is important. If unavoidable circumstances arise which prevent attendance or preparation, the instructor should be advised by email with as much advance notice as possible. Please note that students cannot miss more than two out of 12 course sessions. For further information please consult the Examination Rules §10.\nAcademic Integrity\nThe Hertie School is committed to the standards of good academic and ethical conduct. Any violation of these standards shall be subject to disciplinary action. Plagiarism, deceitful actions as well as free-riding in group work are not tolerated. See Examination Rules §16.\nCompensation for Disadvantages\nIf a student furnishes evidence that he or she is not able to take an examination as required in whole or in part due to disability or permanent illness, the Examination Committee may upon written request approve learning accommodation(s). In this respect, the submission of adequate certificates may be required. See Examination Rules §14.\nExtenuating circumstances\nAn extension can be granted due to extenuating circumstances (i.e., for reasons like illness, personal loss or hardship, or caring duties). In such cases, please contact the course instructors and the Examination Office in advance of the deadline.\nSession Overview\nSession\nDate\nTitle\n1\n08.09.2020\nFoundations\n2\n15.09.2020\nExperiments, Quasi-experiments, and Definitely not Experiments\n\n\nAssignment 1 out\n3\n22.09.2020\nStratification, Regression, and all that\n4\n29.09.2020\nMachine Learning and Big Data Changes Everything\n\n\nAssignment 2 out\n5\n06.10.2020\nEven More Machine Learning\n\n\nAssignment 3 out\n6\n13.10.2020\nHalf Time Review, plus some Diff-in-Diff\n\n\nAssignment 4 out\n\n\n \n7\n27.10.2020\nCollider Bias in Theory and Practice\n8\n03.11.2020\nMediation, of the Statistical Variety\n9\n10.11.2020\nFairness and Bias in Algorithms and Humans\n10\n17.11.2020\nFairness and Bias: Case Study\n11\n24.11.2020\nSpecial topics: Sensitivity and Bounds\n\n\nAssignment 5 out\n12\n01.12.2020\nSpecial Topics: Alternative approaches to Causal Inference\n\n14.12.2020\nFinal Exam Week\n\n\n\n",
      "last_modified": "2021-03-23T09:13:16+01:00"
    },
    {
      "path": "week01.html",
      "title": "Foundations",
      "description": "Mechanisms and Counterfactuals",
      "author": [],
      "date": "September 8th, 2020",
      "contents": "\nThis week we review the counterfactual theory of causation and the kinds of causal inferences it licenses. We’ll compare and contrast two complementary views of a causal mechanism: as a directed acyclic graph (DAG) that connects variables and implies statistical independence relations between them, and as independence relations that connect the potential outcome (PO) for each of these variables that individual cases can realize.\nIt is often helpful to think of POs as the individual level implications of a DAG, where some implications are about what did not happen but could have done had things gone differently.\nIn the background of each way of thinking about causation is the idea that there is a set of specific functions that realize the arrows in the DAG and are the reason that the POs have whatever structure they do. Since we seldom have strong ideas about what these functions look like, it is often useful to ask whether summaries of a functional relationship at particular values of the variables can be identified from data without knowing the functional forms in advance. This is the question of non-parametric causal identification, for which DAGs were designed. We can also ask about the minimal assumptions about functional form required, for example that some relationship is monotonically increasing. For this, switching to POs becomes necessary.\nReadings (for weeks 1 and 2)\nWhatIf ch. 1, 2, 3, 6, and 10\nG. W. Imbens (2020) Potential Outcome and Directed Acyclic GraphApproaches to Causality: Relevance for Empirical Practice in Economics Arxiv 1907.07271v2 (A graph skeptical overview of this week’s topics from an economist)\nSteiner et al. 2017 Graphical models for quasi-experimental designs Sociological Methods and Research.\nLecture\nMoodle page\n\n\n\n",
      "last_modified": "2021-03-23T09:13:16+01:00"
    },
    {
      "path": "week02.html",
      "title": "Experiments, Quasi-experiments, and Definitely not Experiments",
      "description": "What's so great about randomization and control anyway?",
      "author": [],
      "date": "September 8th, 2020",
      "contents": "\nAn important question to ask for any study design is: what causal effects are, or could be identified here? Often the answer is ‘none’, and that is an important thing to know. Not only is purely descriptive research important in itself, but all causal inferences from data are still descriptive studies when the causal claims are removed.\nMore typically we must ask: what assumptions - in the broadest sense - would it be necessary to make in order to identify an effect of interest in this study?\nThis week we revisit the ‘experimental ideal’ for causal inference with a critical eye. In most policy contexts we cannot, or should not, randomize or control ‘treatment’ assignment, so one question is how close we can get to the ideal.\nConversely, when we do manage to get an experiment running, particularly a field experiment, then lots of things can happen that make our happy randomization and control go wrong, e.g. non-compliance, attrition (‘drop out’) and missing data of a less drastic variety. At this point we have an partly ‘observational’ study again, so we may as well get used to it.\nEither way it’ll be useful to work in a framework that doesn’t strongly distinguish experimental and non-experimental work. The general issue will be: how close to the experimental ideal can we get?\nFinally we will consider methods of adaptive experimentation where the sample size and design is dynamically allocated. This methodology was developed in computer science for ‘active’, and more generally ‘reinforcement learning’ applications, but is now used for large scale experimentation, e.g.  in the tech sector. We will look at an example using Thompson sampling.\nReadings (for weeks 1 and 2)\nA. S. Gerber and D. P. Green (2012) ‘Field experiments: Design, analysis, and interpretation’ Norton. (highly recommended for those new to field experiments)\nEGAP’s methods guides on many of the topics from lecture. Well worth bookmarking. Mostly development examples.\nG. W. Imbens (2020) Potential Outcome and Directed Acyclic Graph Approaches to Causality: Relevance for Empirical Practice in Economics Arxiv 1907.07271v2 (A graph skeptical overview of this week’s topics from an economist)\nWhatIf ch. 1, 2, 3, 6, and 10\nSteiner et al. 2017 Graphical models for quasi-experimental designs Sociological Methods and Research.\nRusso et al. 2017 A tutorial on Thompson sampling arXiv 1707.0203.\nLecture\nMoodle page (not there yet)\n\n\n\n",
      "last_modified": "2021-03-23T09:13:17+01:00"
    },
    {
      "path": "week03.html",
      "title": "Stratification, Regression, and all that",
      "description": "When is a regression a good plan for causal inference? Not always, it turns out.",
      "author": [],
      "date": "September 8th, 2020",
      "contents": "\nThis week we cast our critical eye on our old friend, the regression model. It is helpful to think about regression models as a more sophisticated form of stratification, but they do more than that. And also less. Knowing what they actually do from a causal inference perspective makes us a little more cautious, but also more realistic about what regression can offer us.\nIn particular, we may want to use them indirectly, for example to generate weights such as propensity scores, or to combine different regression models in the same analysis, for example to study questions of mediation.\nFor any regression model we’ll also ask want to ask two big questions: First, since not everything that should be conditioned on is in this regression model, how sensitive are causal inferences to what is not observed. Second, should everything that is in the model be there. For example, conditioning on collider variables will wreck the most otherwise careful analysis and, as often, there will be no statistical warning that anything has gone wrong. We’ll consider a general typology of controls, good and bad, and the unwiseness of interpreting the ‘effects’ of control variables.\nWe’ll also ask about the representativeness of regression estimators of causal effects when they are heterogeneous and note that here, perhaps surprisingly, some cases are more influential than others.\nReadings\nWhatIf: ch. 11, 12, 13, 15\nAronow and Samii (2016) Does regression produce representative estimates of causal effects? American Journal of Political Science.\nCinelli et al. (2020) A crash course in good and bad controls\nKeele et al (2020) The causal interpretation of estimated associations in regression models Political Science and Research Methods\nKing and Nielsen (2019), Why Propensity Scores Should Not Be Used for Matching Political Analysis\nLecture\n\n\n\n",
      "last_modified": "2021-03-23T09:13:17+01:00"
    },
    {
      "path": "week04.html",
      "title": "Machine Learning and Big Data Changes Everything",
      "description": "Or maybe it doesn't. Let's see",
      "author": [],
      "date": "September 8th, 2020",
      "contents": "\nThis week we see how our causal inferences could be helped or hindered by taking advantage of more powerful tools for regression from machine learning, and making use of increasingly massive data sets.\nMore powerful models usually means more ‘flexible’ models so it will be important to see what the costs and benefits of non-linear regression are for causal inference. More flexible models allows us to go wrong more drastically, so questions of model fitting and overfitting will be important here.\nBigger data will certainly make our inferences more precise, but it is an open question whether it can make us righter. Identifying a causal effect is the situation where, if we had all the data we could possibly want then some quantity estimated from a model would be the causal effect we are interested in. Conversely, if the effect is not identified then no amount more data will make it so. So we might be skeptical that more precision in our estimation due to more data will help. However, in policy domains we are often interested in a causal effect in some groups more than in others, so more data may help us identify these group effects better. Unless we overfit the model, in which case our precision is an illusion.\nReadings\nWhatIf: ch. 11 and 18\nMeng, X.-L. (2018) Statistical paradises and paradoxes in big data (I): Law of large populations, big data paradox, and the 2016 US presidential election Annals of Applied Statistics.\nWager and Athey (2017) Estimation and Inference of Heterogeneous Treatment Effects using Random Forests Arxiv 1510.04342v4\nChetnozhukov, V. et al. (2018)Double Machine Learning for Causal and Treatment Effects, see also (Felton on https://scholar.princeton.edu/sites/default/files/bstewart/files/felton.chern_.slides.20190318.pdf) for an accessible introduction.\nHernan et al. (2019) Data science is science’s second chance to get causal inference right. A classification of data science tasks ArXiv 1804.10846.\nThree excellent machine learning textbooks\nThese two are freely downloadable\nBishop, C. M. (2006) pattern Recognition and Machine Learning Springer.\nHastie et al. (2009) The Elements of Statistical Learning 2nd Ed. Springer.\nAnd this one not\nMurphy, K. P. (2012) Machine Learning: a Probabilistic Perspective MIT Press\nLecture\nLink\n\n\n\n",
      "last_modified": "2021-03-23T09:13:17+01:00"
    },
    {
      "path": "week05.html",
      "title": "Even More Machine Learning",
      "description": "Because one week was not enough",
      "author": [],
      "date": "September 8th, 2020",
      "contents": "\nBy popular demand - more machine learning. Diff-in-diff will move to next week.\nReadings\nWager and Athey (2017) Estimation and Inference of Heterogeneous Treatment Effects using Random Forests Arxiv 1510.04342v4\nBishop, C. M. (2006) pattern Recognition and Machine Learning Springer.\nLecture\nLink\n\n\n\n",
      "last_modified": "2021-03-23T09:13:18+01:00"
    },
    {
      "path": "week06.html",
      "title": "Old School Tools from the Economists (2)",
      "description": "Half Time Review, plus some Diff-in-Diff",
      "author": [],
      "date": "September 8th, 2020",
      "contents": "\nTaking stock of things now we’re half way through.\nReadings\nFrom the slides\nLecture\nLink\n\n\n\n",
      "last_modified": "2021-03-23T09:13:18+01:00"
    },
    {
      "path": "week07.html",
      "title": "Collider Bias in Theory and Practice",
      "description": "Not everything is a confounder, so do not condition on all the things",
      "author": [],
      "date": "September 8th, 2020",
      "contents": "\nWe spend this week thinking about ‘collider bias’ and its consequences.\nCollider bias provides a particularly illuminating framework for understanding why all the things you were told not to do in your last statistics class were bad. Examples, include selection on the dependent variable, conditioning on post-treatment outcomes, and generally pointing statistical models at data whose provenance you are unsure of.\nCollider bias is a particular issue for policy students as much of their data comes from organizations, who collected it for reasons unrelated but often unexpectedly relevant to the purpose you want it for. In particular, administrative data is usually created in response to some event, for example a birth, doctor’s appointment, accident, or arrest. Colliders will be particulary troublesome when we try to answer some questions with this kind of non-randomly sampled data.\nReadings\nWhatIf ch. 8\nHernan et al. 2004 ‘A Structural Approach to Selection Bias’ Epidemiology.\nElwert and Winship (2014) ‘Endogenous selection bias: The problem of conditioning on a collider variable’ Annual Review of Sociology.\nLecture\nLink\n\n\n\n",
      "last_modified": "2021-03-23T09:13:18+01:00"
    },
    {
      "path": "week08.html",
      "title": "Mediation, of the Statistical Variety",
      "description": "It's harder than you think to learn about causal mechanisms",
      "author": [],
      "date": "September 8th, 2020",
      "contents": "\nThis week we ask the question: if there are several ways to generate a policy outcome, how important is each route? This is the mediation analysis, beloved of psychologists and communication scholars and, we shall see, rather more difficult than they hope.\nOne of the most important questions we can ask when trying to unpack the idea of one ‘route’ to an outcome being more important than another, is: what precisely is the causal effect of interest? Perhaps unexpectedly, the answers to this question require some interesting - or dismaying depending on your preferences - metaphysical considerations. For example, is the effect of a job finding program on labour force outcome more mediated by the increasing confidence or improving skills?\nAssuming for a moment that these are independent, is the causal effect of the confidence ‘route’ the difference between your labour market outcome when you get treated, your confidence is raised, and your skills improved, and the counterfactual you whose skills are improved because you were treated, but whose confidence is not increased, exactly as if you were not treated? And if so, who is this person who is and is not affected by the treatment?\nHere we are only trying to define the causal effects; things get harder as we try to estimate it.\nReadings\nKeele (2015) ‘Causal mediation analysis: Warning! Assumptions ahead’ American Journal of Evaluation. or\nGreen et al. (2010) ‘Enough already about “black box” experiments: Studying mediation is more difficult than most scholars suppose’ The Annals of the American Academy of Political and Social Science. or\nBullock et al. (2010) ‘Yes, but what’s the mechanism? (Don’t expect an easy answer)’ Journal of Personality and Social Psychology. or\nImai et al. (2010) ‘A general approach to causal mediation analysis’ Psychological Methods.\nAcharya et al. (2015) ‘Explaining causal findings without bias: Detecting and assessing direct effects’\nLecture\nLink\n\n\n\n",
      "last_modified": "2021-03-23T09:13:19+01:00"
    },
    {
      "path": "week09.html",
      "title": "Fairness and Bias in Algorithms and Humans",
      "description": "If fairness is a counterfactual concept then lots of things can be biased",
      "author": [],
      "date": "September 8th, 2020",
      "contents": "\nThis week we look at fairness and its evil twin bias. Our starting point will be, roughly, that fairness as an idea is essentially counterfactual. Specifically, I or my algorithm treat you fairly in an allocation decision with respect to one of your attributes, for example your gender, if I would have allocated you the same thing had your gender been different.\nWhile not the only way to think about fairness, this has some intuitive appeal. However, many other things about you would have been caused to be different had your gender been different, even assuming the counterfactual ‘you with another gender’ is conceivable. Things get interestingly difficult rather quickly and much legal and moral argument depends implicitly on answers to the ensuing difficulties, for example the concrete question about whether fairness requires that no allocation model depends on your gender. We will examine circumstances where this simple heuristic may fail as well as situations where competing definitions of fairness are in principle incompatible. We will try to use our causal inference tools to make some sense of the issue.\nReadings\nKusner et al. (2017) ‘Counterfactual Fairness’ Advances in Neural Information Processing Systems.\nRussell et al. (2017) ‘When worlds collide: Integrating different counterfactual assumptions in fairness’ Advances in Neural Information Processing Systems.\nPleiss et al. (2017) ‘On Fairness and Calibration’. Arxiv\nLecture\nLink\n\n\n\n",
      "last_modified": "2021-03-23T09:13:19+01:00"
    },
    {
      "path": "week10.html",
      "title": "Fairness and Bias: Case Study",
      "description": "A deep dive into an important policy question",
      "author": [],
      "date": "September 8th, 2020",
      "contents": "\nThis week we take a in-depth look at the question of whether and if so, to what extent, US police are biased against monorities in their use of force. We will base the session around a recent paper on the topic in the light of its precursors and subsequent controversy.\nThis issue will encompass many of the causal inference issues dealt with in the course, and provides an important policy-relevant example of multiple causal inference tools at work.\nReadings\nKnox et al. 2020 ‘Administrative Records Mask Racially Biased Policing’ American Journal of Political Science.\nGaebler et al. Deconstructing Claims of Post-Treatment Biasin Observational Studies of Discrimination\nKnox et al Can Racial Bias in Policing Be Credibly EstimatedUsing Data Contaminated by Post-Treatment Selection? MS\nLecture\nLink\n\n\n\n",
      "last_modified": "2021-03-23T09:13:19+01:00"
    },
    {
      "path": "week11.html",
      "title": "Special topics: Sensitivity and Bounds",
      "description": "When you just don't know, sometimes you should offer all possible answers",
      "author": [],
      "date": "September 8th, 2020",
      "contents": "\nIn this session we consider how to to gauge the robustness of causal inferences to things we know we don’t know, but suspect might matter. This process is called sensitivity analysis and is not practiced as much as you might hope.\nThis is distinct from what are referred to as ‘robustness checks’ because rather than investigating alternative functional form of or additional measured variables, it asks: how big do the effects of all unmeasured confounders have to be before the effect I currently have disappears.\nWe will also look at tools to bound causal effects. Suprisingly often observed data will imply logical constraints on how big or small a causal effect could possibly be. Applying these can often be informative, even without a traditional causal identification.\nReadings\nCinelli and Hazlett (2020) ‘Making sense of sensitivity: extending omitted variable bias’\nLecture\nLink\n\n\n\n",
      "last_modified": "2021-03-23T09:13:20+01:00"
    },
    {
      "path": "week12.html",
      "title": "Special Topics: Alternative Approaches to Causal Inference",
      "description": "There's more than one way to do it. Well, maybe.",
      "author": [],
      "date": "September 8th, 2020",
      "contents": "\nThis week we will consider some supposed alternatives to the counterfactual theory of causation we have been assuming, and see what the we would do differently under them.\nWe will also consider ‘process tracing’ as a qualitative tool for investigating causal mechanisms, as a precursor to a general discussion of how we might use additional information in our causal inferences, whether qualitative or quantitative.\nReadings\nBeach Process-Tracing Methods in Social Science (old school PT)\nBennett (2008) ‘Process tracing: A Bayesian perspective.’ The Oxford handbook of political methodology\nLecture\nLink\n\n\n\n",
      "last_modified": "2021-03-23T09:13:20+01:00"
    }
  ],
  "collections": []
}
